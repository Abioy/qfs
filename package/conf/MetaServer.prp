#
# $Id#
#
# Copyright 2008-2012 Quantcast Corp.
#
# Author: Mike Ovsiannikov
#
# This file is part of Kosmos File System (KFS).
#
# Licensed under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License. You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
# implied. See the License for the specific language governing
# permissions and limitations under the License.

# Client listener port.
metaServer.clientPort = 20000

# Chunk server listener port.
metaServer.chunkServerPort = 30000

# Meta serve transactions log directory.
metaServer.logDir = meta/transaction_logs

# Meta server checkpoint directory.
metaServer.cpDir = meta/checkpoint

# Allow to automatically create an empty file system if checkpoint file does not
# exists.
# The default is 0, as under the normal circumstances where the file system
# content is of value, completely loosing checkpoint, transaction log, and
# automatically creating an empty fs will have the same effect as conventional
# "mkfs". All chunks (blocks) will get deleted, and restoring the checkpoint,
# and logs later won't be sufficient to recover the data.
metaServer.createEmptyFs = 1

# Root directory permissions -- used only when the new file system created.
# metaServer.rootDirUser  = 0
# metaServer.rootDirGroup = 0
# metaServer.rootDirMode  = 0755

# Defaults for checkpoint and transaction log without permissions conversion on
# startup.
# metaServer.defaultLoadUser     = 0
# metaServer.defaultLoadGroup    = 0
# metaServer.defaultLoadFileMode = 0644
# metaServer.defaultLoadDirMode  = 0755

# The size of the "client" thread pool.
# When set to greater than 0, dedicated threads to do client network io, request
# parsing, and response assembly are created. The thread pool size should
# usually be (at least one) less than the number of cpus. "Client" threads help
# with processing large amount of ["short"] requests where more cpu used for
# context switch, network io, request parsing, and response assembly, than
# the cpu for the request processing itself. For example i-node attribute
# lookup, or write append chunk allocations that can be satisfied from the write
# append allocation cache.
# Default is 0 -- no dedicated "client" threads.
# metaServer.clientThreadCount = 0

# Meta server threads affinity.
# Presently only supported on linux.
# The first cpu index to set thread affinity to.
# The main thread will be assigned to the cpu at the specified index, then the
# next "client" thread will be assigned to the cpu index plus one and so on.
# For example with 2 client threads and start cpu index 0 the threads affinity
# would be 0 1 2 respectively.
# Useful on machines with more than one multi-core processor with shared dram
# cache. Assigning the threads to the same processor might help minimize dram
# cache misses.
# Default is off (start index less than 0) no thread affinity set.
# metaServer.clientThreadStartCpuAffinity = -1

# Meta server process max. locked memory.
# If set to a value greater than 0 then locked memory limit will be set to the
# specified value, and mlock(MCL_CURRENT|MCL_FUTURE) invoked.
# Under linux running under non root user require properly setting locked memory
# hard limit.
# Default is 0 -- no memory locking.
# metaServer.maxLockedMemory = 0

# Size of [network] io buffer pool.
# The default buffer size is 4K, therefore the amount of memory is
# 4K * metaServer.bufferPool.partionBuffers.
# All io buffers are allocated at startup.
# If memory locking enabled io buffers are locked in memory at startup.
# Default is 256K or 1GB on 64 bit system, and 32K or 128MB on 32 bit system.
# metaServer.bufferPool.partionBuffers = 262144

# ==============================================================================
# The parameters below this line can be changed at runtime by editing the
# configuration file and sending meta server process HUP signal.
# Note that to restore parameter to default at run time the default value must
# be explicitly specified in the configuration file, i.e. commenting out the
# parameter will not have any effect until restart.

# Message log level FATAL, ALERT, CRIT, ERROR, WARN, NOTICE, INFO, DEBUG
metaServer.msgLogWriter.logLevel = INFO

# Mininum number of connected / functional chunk servers before the file system
# can be used.
metaServer.minChunkservers = 1

# Wait 30 sec for chunk servers to connect back after restarting, before file
# system considered fully functional.
metaServer.recoveryInterval = 30 

# For write append use the low order bit of the IP address for the chunk servers
# master/slave assignment. This scheme is works well if least significant bit of
# ip address uniformly distributes masters and slaves withing the rack,
# especially with in rack placement for append.
# Default is 0. Assign master / slave to keep number of masters and slaves
# equal. The obvious downside of this is that the assignment depends on the
# server connection order.
# metaServer.assignMasterByIp = 0

# Chunk server executables md5sums white list.
# The chunk server sends its executable md5sum when it connects to the meta
# servrer. If the following space separated list is not empty and does not
# does not contain the chunk server executable checksum then the chunk server
# is instructed to exit or restart itself.
# This might be useful for upgrade or versions control.
# While the chunk server is connected to the meta server no md5 sum verification
# performed.
# Default is empty list.
# metaServer.chunkServerMd5sums =

# Unique file system id -- some name that uniquely identifies distributed file
# system instance.
# This is used to protect data loss / and or corruption in the case where chunk
# server(s) connect to the "wrong" meta server.
# The meta server will not accept connections from the chunk servers with a
# different "cluster key".
# Default is empty string.
metaServer.clusterKey = my-fs-unique-identifier

# Assign rack id by ip prefix -- ip address treated as string.
# The prefix can be positioned with trailing ??
# For example: 10.6.34.2?
# The rack id assigned on chunk server connect, and will not change until the
# chunk server re-connect. Therefore the configuration file changes will not
# have any effect until the chunk servers re-connect.
# Default is empty -- use rack id assigned in the chunk server config.
# metaServer.rackPrefixes =
# Example:
# 10.6.1.* -- rack 1, 10.6.2.* -- rack 2, 10.6.4.1? -- rack 4 etc.
# metaServer.rackPrefixes = 10.6.1. 1  10.6.2. 2  10.6.4.1? 4  10.6.4.1 5

# "Static" placement weights of the racks. The more weight and more chunk
# servers are in the rack the more likely the rack will be chosen by chunk
# allocation.
# Default is empty -- all weight are default to 1.
# metaServer.rackWeights =
# Example: Racks 1 and 2 have weight 1, rack 3 -- 0.9, rack 4 weight 1.2,
# rack 5 weight 1.5. All other rack weights are 1.
# metaServer.rackWeights = 1 1  2 1  3 0.9  4 1.2  5 1.5

# Various timeout settings.

# Extend lease expiration time by 30 sec. in the case of the write master
# disconnect, to give it a chance to re-connect.
# Default is 30 sec.
# metaServer.leaseOwnerDownExpireDelay = 30

# Re-replication or recovery delay in seconds on chunk server down, to give
# chunk server a chance to attempt to re-connect.
# Default is 120 sec.
# metaServer.serverDownReplicationDelay = 120

# Chunk server heartbeat interval.
# Default is 30 sec.
# metaServer.chunkServer.heartbeatInterval = 30

# Chunk server operations timeouts.
# Heartbeat timeout results in declaring chunk server non operational, and
# closing connection.
# All other operations timeout are interpreted as the operation failure.
# The values are in seconds.
# metaServer.chunkServer.heartbeatTimeout    = 60
# metaServer.chunkServer.chunkReallocTimeout = 75
# metaServer.chunkServer.chunkAllocTimeout   = 40
# metaServer.chunkServer.chunkReallocTimeout = 75
# metaServer.chunkServer.makeStableTimeout   = 330
# metaServer.chunkServer.replicationTimeout  = 330

# Other chunk server operations timeout.
# metaServer.chunkServer.requestTimeout      = 600

# Chunk server space utilization placement threshold.
# Chunk servers with space utilization over this threshold are not considered
# as candidates for the chunk placement.
# Default is 0.95 or 95%.
# metaServer.maxSpaceUtilizationThreshold = 0.95

# Unix style permissions
# Space separated list of ip addresses of hosts where root user is allowed.
# Empty list means that root user allowed on any host.
# Default is empty.
# metaServer.rootHosts =

# File modification time update resolution. Increasing the value will reduce the
# transaction log writes with large files.
# Default is 1 sec.
# metaServer.MTimeUpdateResolution = 1

# Force effective user to root. This effectively turns off all permissions
# control.
# Default is off.
# metaServer.forceEUserToRoot = 0

# Client backward compatibility.
# Defaults are no user and no group -- disable backward compatibility.
# metaServer.defaultUser     = 0xFFFFFFFF
# metaServer.defaultGroup    = 0xFFFFFFFF
# metaServer.defaultFileMode = 0644
# metaServer.defaultDirMode  = 0755

# The chunk server disconnects history size. Useful for monitoring.
# Default is 4096 slots / disconnect events.
# metaServer.maxDownServersHistorySize = 4096

# Space and placement re-balancing.
# Space re-balancing controlled by the next two parameters (thresholds) below.
# Re-balancing constantly scans all chunks in the system and checks chunk
# placement withing the replication or RS groups, and moves chunks from chunk
# servers that are above metaServer.maxRebalanceSpaceUtilThreshold to the chunk
# servers that are below metaServer.minRebalanceSpaceUtilThreshold.
# Default is 1 -- on.
# metaServer.rebalancingEnabled = 1

# Space re-balancing thresholds.
# Move chunk from the servers that exceed the
# metaServer.maxRebalanceSpaceUtilThreshold
# Default is 0.82
# metaServer.maxRebalanceSpaceUtilThreshold = 0.82

# Move chunks to server below metaServer.minRebalanceSpaceUtilThreshold.
# Default is 0.72.
# metaServer.minRebalanceSpaceUtilThreshold = 0.72

# Time interval in seconds between replication queues scans.
# The more often the scan is scheduled the more cpu can potentially use.
# Default is 5 sec.
# metaServer.replicationCheckInterval = 5

# Re-balance scan depth.
# Max number of chunks to scan in one partial scan. The more chunks are scanned
# the more cpu re-balance will use, and the "faster" it will scan the chunks.
# metaServer.maxRebalanceScan = 1024

# Single re-balance partial scan time limit.
# Default is 0.03 sec.
# metaServer.maxRebalanceRunTime = 0.03

# Minimum time between two consecutive re-balance partial scans.
# Default is 0.512 sec.
# metaServer.rebalanceRunInterval = 0.512

# ------------------ Chunk placement parameters --------------------------------

# The metaServer.sortCandidatesByLoadAvg and
# metaServer.sortCandidatesBySpaceUtilization are mutially exclusive.
# metaServer.sortCandidatesBySpaceUtilization takes precedence over
# metaServer.sortCandidatesByLoadAvg if both set to 1

# When allocating (placing) a chunk prefer chunk servers with lower "load"
# metric over the chunk servers with the higher "load" metric.
# For the write intesive file systems turning this mode on is
# recommended.
# Default is 0. Do not take chunk server "load" metric into the account.
# metaServer.sortCandidatesByLoadAvg = 0

# When allocating (placing) a chunk prefer chunk servers with lower disk space
# utilizaiton.
# Default is 0. Do not take space utilization into the account.
# metaServer.sortCandidatesBySpaceUtilization = 0

# When allocating (placing) a chunk do not consider chunk server with the "load"
# exceeding average load multiplied by metaServer.maxGoodCandidateLoadRatio.
# Default is 4.
# metaServer.maxGoodCandidateLoadRatio = 4

# When allocating (placing) a chunk do not consider chunk server with the "load"
# exceeding average "master" chunk server load multiplied by
# metaServer.maxGoodMasterLoadRatio if the chunk server is used as master (head
# or synchronous replication chain).
# Default is 4.
# metaServer.maxGoodMasterLoadRatio = 4

# When allocating (placing) a chunk do not consider chunk server with the "load"
# exceeding average "slave" load multiplied by metaServer.maxGoodSlaveLoadRatio
# if the chunk server is used as slave.
# Default is 4.
# metaServer.maxGoodSlaveLoadRatio = 4

# When allocating (placing) a chunk do not consider chunk server with the
# average number of chunks opened for write per drive (disk) exceeding average
# number of chunks opened for write across all disks / chunks servers multiplied
# by metaServer.maxWritesPerDriveRatio.
# Default is 1.5.
# metaServer.maxWritesPerDriveRatio = 1.5

# When allocating (placing) a chunk do not consider chunk server running on the
# same host as writer if the average number of chunks opened for write per drive
# (disk) exceeding average number of chunks opened for write across all disks /
# chunks servers multiplied by metaServer.maxLocalPlacementWeight.
# Default is 1.0.
# metaServer.maxLocalPlacementWeight = 1.0

# "In rack" placement for append and non append chunk allocations.
# Place chunk replicas on the same rack to save cross rack bandwidth at the cost
# of reduced reliability. Useful for temporary / scratch file systems.
# Default is 0.
# metaServer.inRackPlacementForAppend = 0

# "In rack" placement for non append files.
# Default is 0 - place replicas and chunks from the same RS blocks on different
# racks.
# metaServer.inRackPlacement = 0

#-------------------------------------------------------------------------------

# Order chunk replicas locations by the chunk "load average" metric.
# Default is 0. The replicas locations are shuffled randomly.
# metaServer.getAllocOrderServersByLoad = 0

# metaServer.clientSM.auditLogging = 0
# metaServer.auditLogWriter.logFilePrefixes = /home/qfsm0/auditlog/audit.log
# metaServer.auditLogWriter.maxLogFileSize = 1e9
# metaServer.auditLogWriter.maxLogFiles = 10
# metaServer.auditLogWriter.waitMicroSec = 36000e6

# ===================== Chunk servers configuration parameters. ================

# Chunk server log level.
chunkServer.msgLogWriter.logLevel = NOTICE

# Disk io request timeout.
# Default is 270 sec. Production value is 40 sec.
# chunkServer.diskIo.maxIoTimeSec = 270

# Synchronous replication timeouts.
# Record append synchrounous replicaiton timeout.
# Default is 180 sec. Production value is 20 sec.
# chunkServer.recAppender.replicationTimeoutSec = 180
# Write replication timeout.   
# Default is 300 sec. Production value is 20 sec.
# chunkServer.remoteSync.responseTimeoutSec     = 300

# Controls buffered io -- use os file system cache, instead of direct io on the
# os / file systems that support direct io (most file systems on linux).
# Default is off.
# It is conceivable that enabling buffered io might help with short reads for
# the "broadcast" / "web server" type of loads. For the "typical" large io (1MB)
# requests sequential type loads enabling caching will likely lower cluster
# performance due to higher system (os) cpu overhead, and memory contention.
# Default is off.
# chunkServer.bufferedIo = 0

# If sparse files, and in particular chunks aren't used (sequential write only
# for example) the following parameter can be set to 0.
# chunkServer.allowSparseChunks = 1

# The minimal amount of space in bytes that must be available in order for the
# chunk directory to be used for chunk placement (considered as "writable").
# Default is chunk size -- 64MB plus chunk header size 16KB.
# chunkServer.minFsAvailableSpace = 67125248

# The minimal amount of space that must be available in order for the chunk
# directory to be used for chunk placement (considered as "writable"), expressed
# as part total host file system space.
# Default is 0.05 or 5%, or in other words stop using chunk directory when the
# host file system where the chunk directory resides reaches 95% space
# utilization.
# chunkServer.maxSpaceUtilizationThreshold = 0.05

# The "weight" or pending disk io in chunk placement.
# If set to 0 or less the pending io (number of io bytes in the disk queue) has
# no no effect on the placement (choosing chunk directory where to create chunk).
# If weight set to greater than 0, then the average pending io per chunk
# directory (host file system / disk) is calculated, as
# (total_pending_read_bytes * total_pending_read_weight +
# total_pending_write_bytes * total_pending_write_weight) / chunk_directory_count
# Chunk directories with pending_read + pending_write that exceed the value the
# above are taken out of the consideration for placement.
# Default is 0. Typical production value is 1.3
# chunkServer.chunkPlacementPendingReadWeight  = 0
# chunkServer.chunkPlacementPendingWriteWeight = 0

# Averaging interval for the time incoming io requests spend in io buffer wait
# queue. The "average wait time" value used by the meta server for chunk
# placement. The average exponentially decays (IIR filter).
# Default is 20 sec. Typical production value is 8.
# chunkServer.bufferManager.waitingAvgInterval  = 20

# "Not available" directories rescan interval in seconds. Default is 180 sec.
# (see comment in chunk server configuration file).
# chunkServer.dirRecheckInterval = 180
